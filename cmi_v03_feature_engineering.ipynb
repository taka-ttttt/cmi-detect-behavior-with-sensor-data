{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-29T01:50:50.696926Z",
     "iopub.status.busy": "2025-06-29T01:50:50.696623Z",
     "iopub.status.idle": "2025-06-29T01:50:58.915447Z",
     "shell.execute_reply": "2025-06-29T01:50:58.914068Z",
     "shell.execute_reply.started": "2025-06-29T01:50:50.696903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate, GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T01:51:03.769540Z",
     "iopub.status.busy": "2025-06-29T01:51:03.768970Z",
     "iopub.status.idle": "2025-06-29T01:51:03.774993Z",
     "shell.execute_reply": "2025-06-29T01:51:03.773933Z",
     "shell.execute_reply.started": "2025-06-29T01:51:03.769506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = \"input/train.csv\"\n",
    "    train_demographic_path = \"input/train_demographics.csv\"\n",
    "    test_path = \"input/test.csv\"\n",
    "    test_demographic_path = \"input/test_demographics.csv\"\n",
    "\n",
    "    target = \"gesture\"\n",
    "    \n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "\n",
    "    run_optuna = True\n",
    "    n_optuna_trials = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path).reset_index(drop=True)\n",
    "train_d = pd.read_csv(CFG.train_demographic_path).reset_index(drop=True)\n",
    "test = pd.read_csv(CFG.test_path).reset_index(drop=True)\n",
    "test_d = pd.read_csv(CFG.test_demographic_path).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed columns: []\n"
     ]
    }
   ],
   "source": [
    "# Remove low-variance columns\n",
    "def get_low_var_cols(df, threshold=0.95):\n",
    "    return [col for col in df.columns if df[col].nunique() <= 1 or (df[col] == df[col].iloc[0]).mean() > threshold]\n",
    "\n",
    "lowv = get_low_var_cols(train)\n",
    "train = train.drop(columns=lowv)\n",
    "test = test.drop(columns=[c for c in lowv if c in test.columns])\n",
    "\n",
    "print(f\"removed columns: {lowv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target\n",
    "le_gesture = LabelEncoder()\n",
    "train[\"e_gesture\"] = le_gesture.fit_transform(train[\"gesture\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== rot null sequence count ===\n",
      "train: 50\n",
      "test: 0\n"
     ]
    }
   ],
   "source": [
    "# rotが欠損しているsequenceは除外\n",
    "rot_cols = [col for col in train.columns if col.startswith('rot_')]\n",
    "\n",
    "rot_null_row_train = train[rot_cols].isnull().any(axis=1)\n",
    "rot_null_sequence_ids_train = train.loc[rot_null_row_train, 'sequence_id'].unique()\n",
    "rot_null_row_test = test[rot_cols].isnull().any(axis=1)\n",
    "rot_null_sequence_ids_test = test.loc[rot_null_row_test, 'sequence_id'].unique()\n",
    "\n",
    "train = train[~train['sequence_id'].isin(rot_null_sequence_ids_train)].reset_index(drop=True)\n",
    "test = test[~test['sequence_id'].isin(rot_null_sequence_ids_test)].reset_index(drop=True)\n",
    "\n",
    "print(f\"=== rot null sequence count ===\")\n",
    "print(f\"train: {len(rot_null_sequence_ids_train)}\")\n",
    "print(f\"test: {len(rot_null_sequence_ids_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thm null sequence count: 508\n",
      "tof null sequence count: 435\n",
      "imu only seq count: 508\n",
      "full data seq count: 7593\n"
     ]
    }
   ],
   "source": [
    "# thm, tofのいずれか1つでもNaNがあればIMU-only\n",
    "thm_cols = [col for col in train.columns if col.startswith('thm_')]\n",
    "tof_cols = [col for col in train.columns if col.startswith('tof_')]\n",
    "\n",
    "thm_null_row_train = train[thm_cols].isnull().any(axis=1)\n",
    "thm_null_sequence_ids_train = train.loc[thm_null_row_train, 'sequence_id'].unique()\n",
    "\n",
    "print(f\"thm null sequence count: {len(thm_null_sequence_ids_train)}\")\n",
    "\n",
    "tof_null_row_train = train[tof_cols].isnull().any(axis=1)\n",
    "tof_null_sequence_ids_train = train.loc[tof_null_row_train, 'sequence_id'].unique()\n",
    "\n",
    "print(f\"tof null sequence count: {len(tof_null_sequence_ids_train)}\")\n",
    "\n",
    "imu_only_seq_ids = list(set(thm_null_sequence_ids_train) | set(tof_null_sequence_ids_train))\n",
    "full_data_seq_ids = train[~train['sequence_id'].isin(imu_only_seq_ids)]['sequence_id'].unique()\n",
    "\n",
    "# Set thermal and ToF columns to NaN for IMU-only sequences\n",
    "train.loc[train['sequence_id'].isin(imu_only_seq_ids), thm_cols+tof_cols] = np.nan\n",
    "\n",
    "print(f\"imu only seq count: {len(imu_only_seq_ids)}\")\n",
    "print(f\"full data seq count: {len(full_data_seq_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id                   0\n",
       "sequence_type            0\n",
       "sequence_id              0\n",
       "sequence_counter         0\n",
       "subject                  0\n",
       "                     ...  \n",
       "tof_5_v60           571253\n",
       "tof_5_v61           571253\n",
       "tof_5_v62           571253\n",
       "tof_5_v63           571253\n",
       "e_gesture                0\n",
       "Length: 342, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full dataからimu only用のデータを作成\n",
    "train_imu_only = train.copy()\n",
    "train_imu_only[thm_cols+tof_cols] = np.nan\n",
    "\n",
    "train_imu_only.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_set(df: pd.DataFrame, use_extra: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts an enhanced set of features from sensor data for gesture prediction.\n",
    "\n",
    "    Features include:\n",
    "    - Basic statistics (mean, std, min, max, median)\n",
    "    - Advanced statistics (energy, range, quantiles, skew, kurtosis)\n",
    "    - Peak detection for IMU signals\n",
    "    - Frequency band energy from FFT\n",
    "    - Aggregated thermal and ToF features (max/std, rate of change)\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    imu_cols = [c for c in df.columns if any(x in c for x in ['acc_', 'rot_'])]\n",
    "\n",
    "    # Vector magnitudes and jerk features\n",
    "    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
    "    df['rot_mag'] = np.sqrt(df['rot_x']**2 + df['rot_y']**2 + df['rot_z']**2)\n",
    "    df['acc_jerk_x'] = df['acc_x'].diff().fillna(0)\n",
    "    df['acc_jerk_y'] = df['acc_y'].diff().fillna(0)\n",
    "    df['acc_jerk_z'] = df['acc_z'].diff().fillna(0)\n",
    "    imu_cols.extend(['acc_mag', 'rot_mag', 'acc_jerk_x', 'acc_jerk_y', 'acc_jerk_z'])\n",
    "\n",
    "    # Feature Extraction for IMU Sensors\n",
    "    for col in imu_cols:\n",
    "        arr = df[col].values\n",
    "        # Basic Statistics\n",
    "        feats[f'{col}_mean'] = np.mean(arr)\n",
    "        feats[f'{col}_std'] = np.std(arr)\n",
    "        feats[f'{col}_min'] = np.min(arr)\n",
    "        feats[f'{col}_max'] = np.max(arr)\n",
    "        feats[f'{col}_median'] = np.median(arr)\n",
    "        feats[f'{col}_range'] = feats[f'{col}_max'] - feats[f'{col}_min']\n",
    "        # Advanced Statistics\n",
    "        feats[f'{col}_q25'] = np.quantile(arr, 0.25)\n",
    "        feats[f'{col}_q75'] = np.quantile(arr, 0.75)\n",
    "        feats[f'{col}_energy'] = np.sum(arr**2) / len(arr)\n",
    "        feats[f'{col}_skew'] = skew(arr)\n",
    "        feats[f'{col}_kurtosis'] = kurtosis(arr)\n",
    "        # Rolling Window Features\n",
    "        for window in [5, 10, 15, 20, 25, 50]:\n",
    "            rolling_mean = df[col].rolling(window=window, min_periods=1).mean()\n",
    "            rolling_std = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
    "            feats[f'{col}_roll_mean_{window}_mean'] = rolling_mean.mean()\n",
    "            feats[f'{col}_roll_std_{window}_mean'] = rolling_std.mean()\n",
    "        # Frequency Domain Features (FFT)\n",
    "        fft_vals = rfft(arr)\n",
    "        fft_mags = np.abs(fft_vals)\n",
    "        if len(fft_mags) > 1:\n",
    "            feats[f'{col}_fft_mean'] = np.mean(fft_mags)\n",
    "            feats[f'{col}_fft_max'] = np.max(fft_mags)\n",
    "            freqs = rfftfreq(len(arr))\n",
    "            feats[f'{col}_fft_dominant_freq'] = freqs[np.argmax(fft_mags[1:]) + 1] if len(fft_mags) > 1 else 0\n",
    "            # Energy in Frequency Bands\n",
    "            bands = [(0, 2), (2, 5), (5, 10)]\n",
    "            for low, high in bands:\n",
    "                mask = (freqs >= low) & (freqs < high)\n",
    "                feats[f'{col}_fft_energy_{low}_{high}'] = np.sum(fft_mags[mask])\n",
    "        # Peak Detection\n",
    "        peaks, _ = find_peaks(arr, height=0)\n",
    "        feats[f'{col}_num_peaks'] = len(peaks) / len(arr)\n",
    "\n",
    "    # Cross-Sensor Correlations\n",
    "    for (c1, c2) in [('acc_x', 'acc_y'), ('acc_x', 'acc_z'), ('acc_y', 'acc_z'),\n",
    "                     ('rot_x', 'rot_y'), ('rot_x', 'rot_z'), ('rot_y', 'rot_z')]:\n",
    "        if c1 in df.columns and c2 in df.columns:\n",
    "            feats[f'{c1}_{c2}_corr'] = np.corrcoef(df[c1], df[c2])[0, 1]\n",
    "\n",
    "    # Extra Sensor Features (Thermal & ToF)\n",
    "    if use_extra:\n",
    "        thm_cols = [c for c in df.columns if 'thm' in c]\n",
    "        tof_cols = [c for c in df.columns if 'tof' in c]\n",
    "        # Thermal Features\n",
    "        if thm_cols:\n",
    "            thm_data = df[thm_cols].values\n",
    "            feats['thm_max_across_sensors'] = np.nanmax(thm_data, axis=1).mean()\n",
    "            feats['thm_std_across_sensors'] = np.nanstd(thm_data, axis=1).mean()\n",
    "            for col in thm_cols:\n",
    "                feats[f'{col}_mean'] = np.nanmean(df[col])\n",
    "                feats[f'{col}_max'] = np.nanmax(df[col])\n",
    "                feats[f'{col}_diff_mean'] = df[col].diff().abs().mean()\n",
    "        # ToF Features\n",
    "        if tof_cols:\n",
    "            tof_data = df[tof_cols].values\n",
    "            valid_tof = tof_data >= 0\n",
    "            min_tof_per_time = np.min(tof_data, axis=1, where=valid_tof, initial=10000)\n",
    "            feats['tof_min_across_sensors'] = np.mean(min_tof_per_time)\n",
    "            feats['tof_num_valid_sensors'] = np.sum(valid_tof, axis=1).mean()\n",
    "            for col in tof_cols:\n",
    "                valid_vals = df[col][df[col] >= 0]\n",
    "                if valid_vals.size > 0:\n",
    "                    feats[f'{col}_mean'] = np.mean(valid_vals)\n",
    "                    feats[f'{col}_min'] = np.min(valid_vals)\n",
    "                    feats[f'{col}_diff_mean'] = df[col].diff().abs().mean()\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(data: pd.DataFrame, demographics: pd.DataFrame, use_extra: bool) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"Builds feature matrix and returns features with subject groups for CV.\"\"\"\n",
    "    features = []\n",
    "    groups = []\n",
    "    for seq_id, g in data.groupby('sequence_id'):\n",
    "        subj = g['subject'].iloc[0]\n",
    "        groups.append(subj)\n",
    "        feats = feature_set(g, use_extra=use_extra)\n",
    "        feats['sequence_id'] = seq_id\n",
    "        demo_row = demographics[demographics['subject'] == subj]\n",
    "        if not demo_row.empty:\n",
    "            demo = demo_row.iloc[0]\n",
    "            for dcol in ['age', 'sex', 'height_cm', 'handedness']:\n",
    "                feats[dcol] = demo.get(dcol, np.nan)\n",
    "        features.append(feats)\n",
    "    df_feats = pd.DataFrame(features).set_index('sequence_id')\n",
    "    if 'sex' in df_feats.columns:\n",
    "        df_feats['sex'] = df_feats['sex'].map({'M': 1, 'F': 0})\n",
    "    if 'handedness' in df_feats.columns:\n",
    "        df_feats['handedness'] = df_feats['handedness'].map({'R': 1, 'L': 0, 'A': 2})\n",
    "    \n",
    "    return df_feats, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features for IMU-only sequences...\n",
      "Building features for full-sensor sequences...\n",
      "IMU-only features: (8101, 370)\n",
      "Full-sensor features: (7593, 1349)\n"
     ]
    }
   ],
   "source": [
    "# Build separate feature sets for IMU-only and full-sensor data\n",
    "print(\"Building features for IMU-only sequences...\")\n",
    "X_imu, groups_imu = build_features(train_imu_only, train_d, use_extra=False)\n",
    "y_imu = train_imu_only.groupby('sequence_id')['e_gesture'].first().loc[X_imu.index]\n",
    "\n",
    "print(\"Building features for full-sensor sequences...\")\n",
    "X_extra, groups_extra = build_features(train[train['sequence_id'].isin(full_data_seq_ids)], train_d, use_extra=True)\n",
    "y_extra = train.groupby('sequence_id')['e_gesture'].first().loc[X_extra.index]\n",
    "\n",
    "print(f\"IMU-only features: {X_imu.shape}\")\n",
    "print(f\"Full-sensor features: {X_extra.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv(model, X, y, groups, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    fold別詳細結果表示対応クロスバリデーション関数（F1スコア評価版）\n",
    "    最適エポック数での最終学習機能付き\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"=== {model_name} Cross Validation 開始 ===\")\n",
    "    \n",
    "    # 結果保存用\n",
    "    fold_results = []\n",
    "    f1_macro_scores = []\n",
    "    f1_binary_scores = []\n",
    "    final_scores = []\n",
    "    times = []\n",
    "    best_iterations = []  # 最適エポック数記録用\n",
    "    \n",
    "    # 各fold実行\n",
    "    for fold, (train_idx, val_idx) in enumerate(GroupKFold(n_splits=CFG.n_folds).split(X, y, groups)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # データ分割\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # モデル学習（クローンを作成）\n",
    "        fold_model = clone(model)\n",
    "        fold_model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_val, y_val)],\n",
    "                      eval_metric='multi_logloss',\n",
    "                      callbacks=[early_stopping(150, verbose=False)])\n",
    "        \n",
    "        # 最適エポック数を取得（エラーハンドリング付き）\n",
    "        try:\n",
    "            # LightGBMの正しい属性名\n",
    "            best_iteration = fold_model.best_iteration_\n",
    "        except AttributeError:\n",
    "            # early stoppingが動作しなかった場合やバージョン違いの場合\n",
    "            try:\n",
    "                best_iteration = fold_model.best_iteration\n",
    "            except AttributeError:\n",
    "                # デフォルトとしてn_estimatorsを使用\n",
    "                best_iteration = fold_model.n_estimators\n",
    "                print(f\"Warning: Could not get best_iteration for fold {fold}, using n_estimators={best_iteration}\")\n",
    "        \n",
    "        best_iterations.append(best_iteration)\n",
    "        \n",
    "        # 予測\n",
    "        y_pred = fold_model.predict(X_val)\n",
    "        \n",
    "        # 評価指標計算\n",
    "        f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "        f1_binary = f1_score((y_val == 0), (y_pred == 0), average='binary')\n",
    "        final_score = (f1_macro + f1_binary) / 2\n",
    "        \n",
    "        # 実行時間\n",
    "        fold_time = time.time() - start_time\n",
    "        \n",
    "        # 結果保存\n",
    "        f1_macro_scores.append(f1_macro)\n",
    "        f1_binary_scores.append(f1_binary)\n",
    "        final_scores.append(final_score)\n",
    "        times.append(fold_time)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_binary': f1_binary,\n",
    "            'final_score': final_score,\n",
    "            'time': fold_time,\n",
    "            'best_iteration': best_iteration\n",
    "        })\n",
    "        \n",
    "        # fold別結果表示\n",
    "        print(f\"--- Fold {fold} - Final Score: {final_score:.4f} (F1_macro: {f1_macro:.4f}, F1_binary: {f1_binary:.4f}) - Best Iteration: {best_iteration} - Time: {fold_time:.2f} s\")\n",
    "    \n",
    "    # 最適エポック数の計算\n",
    "    optimal_n_estimators = int(np.mean(best_iterations))\n",
    "    std_iterations = np.std(best_iterations)\n",
    "    \n",
    "    # 全体統計\n",
    "    results = {\n",
    "        'fold_results': fold_results,\n",
    "        'f1_macro': {\n",
    "            'mean': np.mean(f1_macro_scores),\n",
    "            'std': np.std(f1_macro_scores),\n",
    "            'scores': f1_macro_scores\n",
    "        },\n",
    "        'f1_binary': {\n",
    "            'mean': np.mean(f1_binary_scores),\n",
    "            'std': np.std(f1_binary_scores),\n",
    "            'scores': f1_binary_scores\n",
    "        },\n",
    "        'final_score': {\n",
    "            'mean': np.mean(final_scores),\n",
    "            'std': np.std(final_scores),\n",
    "            'scores': final_scores\n",
    "        },\n",
    "        'time': {\n",
    "            'mean': np.mean(times),\n",
    "            'total': np.sum(times)\n",
    "        },\n",
    "        'best_iterations': {\n",
    "            'mean': optimal_n_estimators,\n",
    "            'std': std_iterations,\n",
    "            'all': best_iterations\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 全体結果表示\n",
    "    print(f\"\\n{model_name} Overall Results:\")\n",
    "    print(f\"  F1 Macro: {results['f1_macro']['mean']:.6f} ± {results['f1_macro']['std']:.6f}\")\n",
    "    print(f\"  F1 Binary: {results['f1_binary']['mean']:.6f} ± {results['f1_binary']['std']:.6f}\")\n",
    "    print(f\"  Final Score: {results['final_score']['mean']:.6f} ± {results['final_score']['std']:.6f}\")\n",
    "    print(f\"  Best Iterations: {optimal_n_estimators} ± {std_iterations:.1f} (range: {min(best_iterations)}-{max(best_iterations)})\")\n",
    "    print(f\"  Total Time: {results['time']['total']:.2f} s\")\n",
    "    print()\n",
    "    \n",
    "    # 最適エポック数で最終モデル学習\n",
    "    print(f\"Training final {model_name} model on full dataset with optimal n_estimators={optimal_n_estimators}...\")\n",
    "    \n",
    "    # 最終モデルの作成（最適エポック数で設定）\n",
    "    final_model = clone(model)\n",
    "    \n",
    "    # LightGBMの場合、n_estimatorsを最適値に更新\n",
    "    if hasattr(final_model, 'set_params'):\n",
    "        final_model.set_params(n_estimators=optimal_n_estimators)\n",
    "    else:\n",
    "        # set_paramsが使えない場合の代替案\n",
    "        final_model.n_estimators = optimal_n_estimators\n",
    "    \n",
    "    # 全データで学習（early stoppingなし）\n",
    "    final_model.fit(X, y)\n",
    "    \n",
    "    # モデル保存\n",
    "    model_filename = f\"models/model_{model_name.lower().replace('-', '_').replace(' ', '_')}.pkl\"\n",
    "    joblib.dump(final_model, model_filename)\n",
    "    print(f\"Model saved to: {model_filename}\")\n",
    "    print(f\"Final model trained with n_estimators={optimal_n_estimators}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM (gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_lgbm_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    metric='multi_logloss',\n",
    "    boosting_type='gbdt',\n",
    "    device='gpu',\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=0,\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.015,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_fraction=0.7,\n",
    "    bagging_freq=1,\n",
    "    lambda_l1=0.3,\n",
    "    lambda_l2=0.3,\n",
    "    max_depth=-1,\n",
    "    num_leaves=60,\n",
    "    min_child_samples=20,\n",
    "    n_jobs=-1,\n",
    "    random_state=CFG.seed,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "full_lgbm_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    metric='multi_logloss',\n",
    "    boosting_type='gbdt',\n",
    "    device='gpu',\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=0,\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.015,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_fraction=0.7,\n",
    "    bagging_freq=1,\n",
    "    lambda_l1=0.3,\n",
    "    lambda_l2=0.3,\n",
    "    max_depth=-1,\n",
    "    num_leaves=60,\n",
    "    min_child_samples=20,\n",
    "    n_jobs=-1,\n",
    "    random_state=CFG.seed,\n",
    "    verbose=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMU-only-LightGBM Cross Validation 開始 ===\n",
      "--- Fold 0 - Final Score: 0.5325 (F1_macro: 0.5710, F1_binary: 0.4939) - Best Iteration: 545 - Time: 206.43 s\n",
      "--- Fold 1 - Final Score: 0.5870 (F1_macro: 0.6308, F1_binary: 0.5432) - Best Iteration: 604 - Time: 214.54 s\n",
      "--- Fold 2 - Final Score: 0.5348 (F1_macro: 0.6122, F1_binary: 0.4574) - Best Iteration: 468 - Time: 177.36 s\n",
      "--- Fold 3 - Final Score: 0.5188 (F1_macro: 0.5751, F1_binary: 0.4626) - Best Iteration: 604 - Time: 200.94 s\n",
      "--- Fold 4 - Final Score: 0.5418 (F1_macro: 0.5787, F1_binary: 0.5049) - Best Iteration: 422 - Time: 165.17 s\n",
      "\n",
      "IMU-only-LightGBM Overall Results:\n",
      "  F1 Macro: 0.593569 ± 0.023690\n",
      "  F1 Binary: 0.492408 ± 0.031160\n",
      "  Final Score: 0.542989 ± 0.023247\n",
      "  Best Iterations: 528 ± 73.0 (range: 422-604)\n",
      "  Total Time: 964.44 s\n",
      "\n",
      "Training final IMU-only-LightGBM model on full dataset with optimal n_estimators=528...\n",
      "Model saved to: models/model_imu_only_lightgbm.pkl\n",
      "Final model trained with n_estimators=528\n"
     ]
    }
   ],
   "source": [
    "# Train IMU-only model\n",
    "imu_lgbm_results = train_cv(\n",
    "    model=imu_lgbm_model,\n",
    "    X=X_imu,\n",
    "    y=y_imu,\n",
    "    groups=groups_imu,\n",
    "    model_name=\"IMU-only-LightGBM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full-sensor-LightGBM Cross Validation 開始 ===\n",
      "--- Fold 0 - Final Score: 0.6794 (F1_macro: 0.6540, F1_binary: 0.7048) - Best Iteration: 630 - Time: 336.60 s\n",
      "--- Fold 1 - Final Score: 0.6581 (F1_macro: 0.6799, F1_binary: 0.6364) - Best Iteration: 540 - Time: 348.19 s\n",
      "--- Fold 2 - Final Score: 0.6690 (F1_macro: 0.6440, F1_binary: 0.6941) - Best Iteration: 971 - Time: 1933.40 s\n",
      "--- Fold 3 - Final Score: 0.7240 (F1_macro: 0.6932, F1_binary: 0.7549) - Best Iteration: 731 - Time: 479.17 s\n",
      "--- Fold 4 - Final Score: 0.6986 (F1_macro: 0.6801, F1_binary: 0.7171) - Best Iteration: 702 - Time: 847.68 s\n",
      "\n",
      "Full-sensor-LightGBM Overall Results:\n",
      "  F1 Macro: 0.670228 ± 0.018291\n",
      "  F1 Binary: 0.701454 ± 0.038474\n",
      "  Final Score: 0.685841 ± 0.023309\n",
      "  Best Iterations: 714 ± 144.1 (range: 540-971)\n",
      "  Total Time: 3945.04 s\n",
      "\n",
      "Training final Full-sensor-LightGBM model on full dataset with optimal n_estimators=714...\n",
      "Model saved to: models/model_full_sensor_lightgbm.pkl\n",
      "Final model trained with n_estimators=714\n"
     ]
    }
   ],
   "source": [
    "# Train full-sensor model\n",
    "full_lgbm_results = train_cv(\n",
    "    model=full_lgbm_model,\n",
    "    X=X_extra,\n",
    "    y=y_extra,\n",
    "    groups=groups_extra,\n",
    "    model_name=\"Full-sensor-LightGBM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/le_gesture.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save label encoder\n",
    "joblib.dump(le_gesture, \"models/le_gesture.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_extra_sensors(df):\n",
    "    \"\"\"Detect if sequence has valid thermal/ToF sensor data\"\"\"\n",
    "    thm_cols = [c for c in df.columns if 'thm_' in c]\n",
    "    tof_cols = [c for c in df.columns if 'tof_' in c]\n",
    "    \n",
    "    if not thm_cols or not tof_cols:\n",
    "        return False\n",
    "    \n",
    "    thm_vals = df[thm_cols].values\n",
    "    tof_vals = df[tof_cols].values\n",
    "    \n",
    "    # Check if all values are missing/invalid\n",
    "    if np.all(np.isnan(thm_vals)) and np.all((tof_vals == -1) | (np.isnan(tof_vals))):\n",
    "        return False\n",
    "    \n",
    "    # Return True if sufficient valid data exists\n",
    "    return (np.isnan(thm_vals).mean() < 0.8) or (np.all(tof_vals != -1) and (tof_vals != -1).mean() > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_test(sequence_df: pd.DataFrame, demo_df: pd.DataFrame):\n",
    "    \"\"\"Extract features for test sequence and determine sensor availability\"\"\"\n",
    "    use_extra = has_extra_sensors(sequence_df)\n",
    "    feats = feature_set(sequence_df, use_extra=use_extra)\n",
    "    \n",
    "    # Add demographics\n",
    "    demo_row = demo_df.iloc[0] if len(demo_df) > 0 else {}\n",
    "    for dcol in ['age','adult_child','sex','handedness','height_cm','shoulder_to_wrist_cm','elbow_to_wrist_cm']:\n",
    "        feats[dcol] = demo_row.get(dcol, np.nan)\n",
    "    feats['sex_F'] = int(feats.get('sex', 0) == 0)\n",
    "    feats['sex_M'] = int(feats.get('sex', 0) == 1)\n",
    "    feats['handed_L'] = int(feats.get('handedness', 0) == 0)\n",
    "    feats['handed_R'] = int(feats.get('handedness', 0) == 1)\n",
    "    feats['handed_A'] = int(feats.get('handedness', 0) == 2)\n",
    "    \n",
    "    return pd.DataFrame([feats]), use_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル・エンコーダを読み込む\n",
    "model_imu = joblib.load(\"models/model_imu-only-lightgbm.pkl\")\n",
    "model_extra = joblib.load(\"models/model_full-sensor-lightgbm.pkl\")\n",
    "le_gesture = joblib.load(\"models/le_gesture.pkl\")\n",
    "\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    \"\"\"Smart prediction using appropriate model based on available sensors\"\"\"\n",
    "    sequence = sequence.to_pandas()\n",
    "    demographics = demographics.to_pandas()\n",
    "    \n",
    "    try:\n",
    "        feats, use_extra = extract_features_for_test(sequence, demographics)\n",
    "        model = model_extra if use_extra else model_imu\n",
    "        pred = model.predict(feats)[0]\n",
    "        return le_gesture.inverse_transform([pred])[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return le_gesture.classes_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Server Setup \n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            'input/test.csv',\n",
    "            'input/test_demographics.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>gesture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEQ_000001</td>\n",
       "      <td>Above ear - pull hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEQ_000011</td>\n",
       "      <td>Above ear - pull hair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sequence_id                gesture\n",
       "0  SEQ_000001  Above ear - pull hair\n",
       "1  SEQ_000011  Above ear - pull hair"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('submission.parquet').reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11418275,
     "sourceId": 96164,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
