{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-29T01:50:50.696926Z",
     "iopub.status.busy": "2025-06-29T01:50:50.696623Z",
     "iopub.status.idle": "2025-06-29T01:50:58.915447Z",
     "shell.execute_reply": "2025-06-29T01:50:58.914068Z",
     "shell.execute_reply.started": "2025-06-29T01:50:50.696903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate, GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import glob\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T01:51:03.769540Z",
     "iopub.status.busy": "2025-06-29T01:51:03.768970Z",
     "iopub.status.idle": "2025-06-29T01:51:03.774993Z",
     "shell.execute_reply": "2025-06-29T01:51:03.773933Z",
     "shell.execute_reply.started": "2025-06-29T01:51:03.769506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = \"input/train.csv\"\n",
    "    train_demographic_path = \"input/train_demographics.csv\"\n",
    "    test_path = \"input/test.csv\"\n",
    "    test_demographic_path = \"input/test_demographics.csv\"\n",
    "\n",
    "    target = \"gesture\"\n",
    "    features_train_only = ['sequence_type', 'orientation']\n",
    "\n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "\n",
    "    run_optuna = True\n",
    "    n_optuna_trials = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T01:51:23.225969Z",
     "iopub.status.busy": "2025-06-29T01:51:23.225489Z",
     "iopub.status.idle": "2025-06-29T01:51:23.237779Z",
     "shell.execute_reply": "2025-06-29T01:51:23.235984Z",
     "shell.execute_reply.started": "2025-06-29T01:51:23.225942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(dataframe, dataset):    \n",
    "    print('Reducing memory usage for:', dataset)\n",
    "    initial_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in dataframe.columns:\n",
    "        col_type = dataframe[col].dtype\n",
    "\n",
    "        # 数値型の列のみ処理\n",
    "        if np.issubdtype(col_type, np.number):\n",
    "            c_min = dataframe[col].min()\n",
    "            c_max = dataframe[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float32)\n",
    "                else:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float64)\n",
    "\n",
    "    final_mem_usage = dataframe.memory_usage().sum() / 1024**2\n",
    "    print('--- Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n",
    "    print('--- Memory usage after: {:.2f} MB'.format(final_mem_usage))\n",
    "    print('--- Decreased memory usage by {:.1f}%\\n'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path).reset_index(drop=True)\n",
    "train_d = pd.read_csv(CFG.train_demographic_path).reset_index(drop=True)\n",
    "test = pd.read_csv(CFG.test_path).reset_index(drop=True)\n",
    "test_d = pd.read_csv(CFG.test_demographic_path).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_reduced = train.copy()\n",
    "# train_reduced = reduce_mem_usage(train_reduced, \"train\")\n",
    "# test_reduced = test.copy()\n",
    "# test_reduced = reduce_mem_usage(test_reduced, \"test\")\n",
    "\n",
    "# X = train_reduced.drop(columns=CFG.target, axis=1)\n",
    "# y = train_reduced[CFG.target]\n",
    "# X_test = test_reduced\n",
    "\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4075 IMU-only and 4076 full-sensor sequences\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values per sequence\n",
    "train = train.groupby(\"sequence_id\").apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
    "test = test.groupby(\"sequence_id\").apply(lambda g: g.ffill().bfill()).reset_index(drop=True)\n",
    "\n",
    "# Remove low-variance columns\n",
    "def get_low_var_cols(df, threshold=0.95):\n",
    "    return [col for col in df.columns if df[col].nunique() <= 1 or (df[col] == df[col].iloc[0]).mean() > threshold]\n",
    "\n",
    "lowv = get_low_var_cols(train)\n",
    "train = train.drop(columns=lowv)\n",
    "test = test.drop(columns=[c for c in lowv if c in test.columns])\n",
    "\n",
    "# Encode target\n",
    "le_gesture = LabelEncoder()\n",
    "train[\"e_gesture\"] = le_gesture.fit_transform(train[\"gesture\"])\n",
    "\n",
    "#  Simulate IMU-only scenarios in 50% of training data \n",
    "all_seq_ids = train['sequence_id'].unique()\n",
    "imu_only_seq_ids = np.random.choice(all_seq_ids, size=int(0.5*len(all_seq_ids)), replace=False)\n",
    "extra_seq_ids = [sid for sid in all_seq_ids if sid not in imu_only_seq_ids]\n",
    "\n",
    "# Set thermal and ToF columns to NaN for IMU-only sequences\n",
    "thm_cols = [c for c in train.columns if 'thm_' in c]\n",
    "tof_cols = [c for c in train.columns if 'tof_' in c]\n",
    "train.loc[train['sequence_id'].isin(imu_only_seq_ids), thm_cols+tof_cols] = np.nan\n",
    "\n",
    "print(f\"Created {len(imu_only_seq_ids)} IMU-only and {len(extra_seq_ids)} full-sensor sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_set(df, use_extra=False):\n",
    "    \"\"\"Extract comprehensive features from sensor data\"\"\"\n",
    "    imu_cols = [c for c in df.columns if any(x in c for x in ['acc_', 'rot_'])]\n",
    "    feats = {}\n",
    "    \n",
    "    # IMU features with  statistics\n",
    "    for col in imu_cols:\n",
    "        arr = df[col].values\n",
    "        feats[f'{col}_mean'] = np.mean(arr)\n",
    "        feats[f'{col}_std'] = np.std(arr)\n",
    "        feats[f'{col}_min'] = np.min(arr)\n",
    "        feats[f'{col}_max'] = np.max(arr)\n",
    "        feats[f'{col}_median'] = np.median(arr)\n",
    "        # Additional statistical features\n",
    "        feats[f'{col}_energy'] = np.sum(arr**2) / len(arr)\n",
    "        feats[f'{col}_first'] = arr[0]\n",
    "        feats[f'{col}_last'] = arr[-1]\n",
    "        feats[f'{col}_range'] = np.max(arr) - np.min(arr)\n",
    "        feats[f'{col}_mad'] = np.mean(np.abs(np.diff(arr)))  # Mean absolute difference\n",
    "        feats[f'{col}_zcr'] = ((arr[:-1]*arr[1:]) < 0).mean()  # Zero crossing rate\n",
    "    \n",
    "    # Cross-sensor correlations\n",
    "    for (c1, c2) in [('acc_x','acc_y'),('acc_x','acc_z'),('acc_y','acc_z'),\n",
    "                     ('rot_x','rot_y'),('rot_x','rot_z'),('rot_y','rot_z')]:\n",
    "        if c1 in df.columns and c2 in df.columns:\n",
    "            feats[f'{c1}_{c2}_corr'] = np.corrcoef(df[c1], df[c2])[0,1]\n",
    "    \n",
    "    #  extra sensor features\n",
    "    if use_extra:\n",
    "        # Thermal features\n",
    "        thm_cols = [c for c in df.columns if 'thm_' in c]\n",
    "        for col in thm_cols:\n",
    "            arr = df[col].values\n",
    "            feats[f'{col}_mean'] = np.nanmean(arr)\n",
    "            feats[f'{col}_std'] = np.nanstd(arr)\n",
    "            feats[f'{col}_min'] = np.nanmin(arr)\n",
    "            feats[f'{col}_max'] = np.nanmax(arr)\n",
    "        \n",
    "        #  ToF features\n",
    "        tof_cols = [c for c in df.columns if 'tof_' in c]\n",
    "        if tof_cols:\n",
    "            all_tof = np.stack([df[c].values for c in tof_cols], axis=1)\n",
    "            valid_mask = (all_tof >= 0)\n",
    "            valid_vals = all_tof[valid_mask]\n",
    "            feats['tof_count'] = valid_mask.sum()\n",
    "            feats['tof_valid_mean'] = np.nanmean(valid_vals) if valid_vals.size > 0 else np.nan\n",
    "            feats['tof_valid_std']  = np.nanstd(valid_vals) if valid_vals.size > 0 else np.nan\n",
    "            feats['tof_valid_min']  = np.nanmin(valid_vals) if valid_vals.size > 0 else np.nan\n",
    "            feats['tof_valid_max']  = np.nanmax(valid_vals) if valid_vals.size > 0 else np.nan\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(data, demographics, use_extra=False):\n",
    "    \"\"\"Build feature matrix for training\"\"\"\n",
    "    features = []\n",
    "    for seq_id, g in data.groupby('sequence_id'):\n",
    "        feats = feature_set(g, use_extra=use_extra)\n",
    "        feats['sequence_id'] = seq_id\n",
    "        \n",
    "        # Add demographics\n",
    "        subj = g['subject'].iloc[0]\n",
    "        demo = demographics[demographics['subject']==subj].iloc[0] if (demographics['subject']==subj).any() else {}\n",
    "        for dcol in ['age','adult_child','sex','handedness','height_cm','shoulder_to_wrist_cm','elbow_to_wrist_cm']:\n",
    "            feats[dcol] = demo[dcol] if dcol in demo else np.nan\n",
    "        feats['sex_F'] = int(feats.get('sex',0)==0)\n",
    "        feats['sex_M'] = int(feats.get('sex',0)==1)\n",
    "        feats['handed_L'] = int(feats.get('handedness',0)==0)\n",
    "        feats['handed_R'] = int(feats.get('handedness',0)==1)\n",
    "        features.append(feats)\n",
    "    \n",
    "    return pd.DataFrame(features).set_index('sequence_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features for IMU-only sequences...\n",
      "Building features for full-sensor sequences...\n",
      "IMU-only features: (4075, 94)\n",
      "Full-sensor features: (4076, 119)\n"
     ]
    }
   ],
   "source": [
    "# Build separate feature sets for IMU-only and full-sensor data\n",
    "print(\"Building features for IMU-only sequences...\")\n",
    "X_imu = build_features(train[train['sequence_id'].isin(imu_only_seq_ids)], train_d, use_extra=False)\n",
    "y_imu = train.groupby('sequence_id')['e_gesture'].first().loc[X_imu.index]\n",
    "\n",
    "print(\"Building features for full-sensor sequences...\")\n",
    "X_extra = build_features(train[train['sequence_id'].isin(extra_seq_ids)], train_d, use_extra=True)\n",
    "y_extra = train.groupby('sequence_id')['e_gesture'].first().loc[X_extra.index]\n",
    "\n",
    "print(f\"IMU-only features: {X_imu.shape}\")\n",
    "print(f\"Full-sensor features: {X_extra.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通設定\n",
    "kf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(model, X, y, model_name=\"Model\", cv_folds=None):\n",
    "    \"\"\"\n",
    "    fold別詳細結果表示対応クロスバリデーション関数（F1スコア評価版）\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn compatible model\n",
    "        学習させるモデル\n",
    "    X : pandas.DataFrame\n",
    "        特徴量データ\n",
    "    y : pandas.Series\n",
    "        目的変数\n",
    "    model_name : str\n",
    "        モデル名\n",
    "    cv_folds : KFold object\n",
    "        クロスバリデーション設定\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : CV結果の辞書\n",
    "    \"\"\"\n",
    "    \n",
    "    if cv_folds is None:\n",
    "        cv_folds = KFold(n_splits=CFG.n_folds, shuffle=False, random_state=None)\n",
    "    \n",
    "    print(f\"=== {model_name} Cross Validation 開始 ===\")\n",
    "    \n",
    "    # 結果保存用\n",
    "    fold_results = []\n",
    "    f1_macro_scores = []\n",
    "    f1_binary_scores = []\n",
    "    final_scores = []\n",
    "    times = []\n",
    "    \n",
    "    # 各fold実行\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv_folds.split(X, y)):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # データ分割\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # モデル学習\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 予測\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # 評価指標計算（train_cv関数と同じ方法）\n",
    "        f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "        f1_binary = f1_score((y_val == 0), (y_pred == 0), average='binary')\n",
    "        final_score = (f1_macro + f1_binary) / 2\n",
    "        \n",
    "        # 実行時間\n",
    "        fold_time = time.time() - start_time\n",
    "        \n",
    "        # 結果保存\n",
    "        f1_macro_scores.append(f1_macro)\n",
    "        f1_binary_scores.append(f1_binary)\n",
    "        final_scores.append(final_score)\n",
    "        times.append(fold_time)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_binary': f1_binary,\n",
    "            'final_score': final_score,\n",
    "            'time': fold_time\n",
    "        })\n",
    "        \n",
    "        # fold別結果表示\n",
    "        print(f\"--- Fold {fold} - Final Score: {final_score:.4f} (F1_macro: {f1_macro:.4f}, F1_binary: {f1_binary:.4f}) - Time: {fold_time:.2f} s\")\n",
    "    \n",
    "    # 全体統計\n",
    "    results = {\n",
    "        'fold_results': fold_results,\n",
    "        'f1_macro': {\n",
    "            'mean': np.mean(f1_macro_scores),\n",
    "            'std': np.std(f1_macro_scores),\n",
    "            'scores': f1_macro_scores\n",
    "        },\n",
    "        'f1_binary': {\n",
    "            'mean': np.mean(f1_binary_scores),\n",
    "            'std': np.std(f1_binary_scores),\n",
    "            'scores': f1_binary_scores\n",
    "        },\n",
    "        'final_score': {\n",
    "            'mean': np.mean(final_scores),\n",
    "            'std': np.std(final_scores),\n",
    "            'scores': final_scores\n",
    "        },\n",
    "        'time': {\n",
    "            'mean': np.mean(times),\n",
    "            'total': np.sum(times)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 全体結果表示\n",
    "    print(f\"\\n{model_name} Overall Results:\")\n",
    "    print(f\"  F1 Macro: {results['f1_macro']['mean']:.6f} ± {results['f1_macro']['std']:.6f}\")\n",
    "    print(f\"  F1 Binary: {results['f1_binary']['mean']:.6f} ± {results['f1_binary']['std']:.6f}\")\n",
    "    print(f\"  Final Score: {results['final_score']['mean']:.6f} ± {results['final_score']['std']:.6f}\")\n",
    "    print(f\"  Total Time: {results['time']['total']:.2f} s\")\n",
    "    print()\n",
    "    \n",
    "    # 全データでモデルを学習し直してから保存\n",
    "    print(f\"Training final {model_name} model on full dataset...\")\n",
    "    final_model = clone(model)\n",
    "    final_model.fit(X, y)\n",
    "    \n",
    "    # モデル保存\n",
    "    model_filename = f\"models/model_{model_name.lower()}.pkl\"\n",
    "    joblib.dump(final_model, model_filename)\n",
    "    print(f\"Model saved to: {model_filename}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM (gbdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_lgbm_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    metric='multi_logloss',\n",
    "    device='gpu',\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=0,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1500,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=2,\n",
    "    lambda_l1=0.1,\n",
    "    lambda_l2=0.1,\n",
    "    random_state=CFG.seed,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "full_lgbm_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    metric='multi_logloss',\n",
    "    device='gpu',\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=0,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1500,\n",
    "    num_leaves=50,\n",
    "    max_depth=-1,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=2,\n",
    "    lambda_l1=0.1,\n",
    "    lambda_l2=0.1,\n",
    "    random_state=CFG.seed,\n",
    "    verbose=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMU-only-LightGBM Cross Validation 開始 ===\n",
      "--- Fold 0 - Final Score: 0.5084 (F1_macro: 0.5127, F1_binary: 0.5042) - Time: 57.17 s\n",
      "--- Fold 1 - Final Score: 0.4787 (F1_macro: 0.4924, F1_binary: 0.4651) - Time: 61.00 s\n",
      "--- Fold 2 - Final Score: 0.4713 (F1_macro: 0.4932, F1_binary: 0.4493) - Time: 60.17 s\n",
      "--- Fold 3 - Final Score: 0.4658 (F1_macro: 0.4933, F1_binary: 0.4384) - Time: 61.83 s\n",
      "--- Fold 4 - Final Score: 0.4848 (F1_macro: 0.4737, F1_binary: 0.4960) - Time: 58.35 s\n",
      "\n",
      "IMU-only-LightGBM Overall Results:\n",
      "  F1 Macro: 0.493052 ± 0.012338\n",
      "  F1 Binary: 0.470590 ± 0.025685\n",
      "  Final Score: 0.481821 ± 0.014795\n",
      "  Total Time: 298.53 s\n",
      "\n",
      "Training final IMU-only-LightGBM model on full dataset...\n",
      "Model saved to: models/model_imu-only-lightgbm.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train IMU-only model\n",
    "imu_lgbm_results = run_cross_validation(\n",
    "    model=imu_lgbm_model,\n",
    "    X=X_imu,\n",
    "    y=y_imu,\n",
    "    model_name=\"IMU-only-LightGBM\",\n",
    "    cv_folds=kf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full-sensor-LightGBM Cross Validation 開始 ===\n",
      "--- Fold 0 - Final Score: 0.6434 (F1_macro: 0.5606, F1_binary: 0.7261) - Time: 45.76 s\n",
      "--- Fold 1 - Final Score: 0.6278 (F1_macro: 0.5996, F1_binary: 0.6560) - Time: 43.63 s\n",
      "--- Fold 2 - Final Score: 0.5996 (F1_macro: 0.5957, F1_binary: 0.6034) - Time: 44.63 s\n",
      "--- Fold 3 - Final Score: 0.6164 (F1_macro: 0.5929, F1_binary: 0.6400) - Time: 43.71 s\n",
      "--- Fold 4 - Final Score: 0.6029 (F1_macro: 0.5762, F1_binary: 0.6296) - Time: 45.66 s\n",
      "\n",
      "Full-sensor-LightGBM Overall Results:\n",
      "  F1 Macro: 0.585007 ± 0.014573\n",
      "  F1 Binary: 0.651039 ± 0.041246\n",
      "  Final Score: 0.618023 ± 0.016172\n",
      "  Total Time: 223.39 s\n",
      "\n",
      "Training final Full-sensor-LightGBM model on full dataset...\n",
      "Model saved to: models/model_full-sensor-lightgbm.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train full-sensor model\n",
    "full_lgbm_results = run_cross_validation(\n",
    "    model=full_lgbm_model,\n",
    "    X=X_extra,\n",
    "    y=y_extra,\n",
    "    model_name=\"Full-sensor-LightGBM\",\n",
    "    cv_folds=kf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/le_gesture.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save label encoder\n",
    "joblib.dump(le_gesture, \"models/le_gesture.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_extra_sensors(df):\n",
    "    \"\"\"Detect if sequence has valid thermal/ToF sensor data\"\"\"\n",
    "    thm_cols = [c for c in df.columns if 'thm_' in c]\n",
    "    tof_cols = [c for c in df.columns if 'tof_' in c]\n",
    "    \n",
    "    if not thm_cols or not tof_cols:\n",
    "        return False\n",
    "    \n",
    "    thm_vals = df[thm_cols].values\n",
    "    tof_vals = df[tof_cols].values\n",
    "    \n",
    "    # Check if all values are missing/invalid\n",
    "    if np.all(np.isnan(thm_vals)) and np.all((tof_vals == -1) | (np.isnan(tof_vals))):\n",
    "        return False\n",
    "    \n",
    "    # Return True if sufficient valid data exists\n",
    "    return (np.isnan(thm_vals).mean() < 0.8) or (np.all(tof_vals != -1) and (tof_vals != -1).mean() > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_test(sequence_df: pd.DataFrame, demo_df: pd.DataFrame):\n",
    "    \"\"\"Extract features for test sequence and determine sensor availability\"\"\"\n",
    "    use_extra = has_extra_sensors(sequence_df)\n",
    "    feats = feature_set(sequence_df, use_extra=use_extra)\n",
    "    \n",
    "    # Add demographics\n",
    "    demo_row = demo_df.iloc[0] if len(demo_df) > 0 else {}\n",
    "    for dcol in ['age','adult_child','sex','handedness','height_cm','shoulder_to_wrist_cm','elbow_to_wrist_cm']:\n",
    "        feats[dcol] = demo_row.get(dcol, np.nan)\n",
    "    feats['sex_F'] = int(feats.get('sex', 0) == 0)\n",
    "    feats['sex_M'] = int(feats.get('sex', 0) == 1)\n",
    "    feats['handed_L'] = int(feats.get('handedness', 0) == 0)\n",
    "    feats['handed_R'] = int(feats.get('handedness', 0) == 1)\n",
    "    \n",
    "    return pd.DataFrame([feats]), use_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル・エンコーダを読み込む\n",
    "model_imu = joblib.load(\"models/model_imu-only-lightgbm.pkl\")\n",
    "model_extra = joblib.load(\"models/model_full-sensor-lightgbm.pkl\")\n",
    "le_gesture = joblib.load(\"models/le_gesture.pkl\")\n",
    "\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    \"\"\"Smart prediction using appropriate model based on available sensors\"\"\"\n",
    "    sequence = sequence.to_pandas()\n",
    "    demographics = demographics.to_pandas()\n",
    "    \n",
    "    try:\n",
    "        feats, use_extra = extract_features_for_test(sequence, demographics)\n",
    "        model = model_extra if use_extra else model_imu\n",
    "        pred = model.predict(feats)[0]\n",
    "        return le_gesture.inverse_transform([pred])[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return le_gesture.classes_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Server Setup \n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            'input/test.csv',\n",
    "            'input/test_demographics.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>gesture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEQ_000001</td>\n",
       "      <td>Above ear - pull hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEQ_000011</td>\n",
       "      <td>Above ear - pull hair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sequence_id                gesture\n",
       "0  SEQ_000001  Above ear - pull hair\n",
       "1  SEQ_000011  Above ear - pull hair"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('submission.parquet').reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11418275,
     "sourceId": 96164,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
